{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5ae033-c541-4ab5-bb73-b860420a193d",
   "metadata": {},
   "source": [
    "# **IBM Data Science Professional Certificate**\n",
    "\n",
    "Course 7: Data Analysis with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24231ae-ef04-41ca-8cd8-78b2ec43db50",
   "metadata": {},
   "source": [
    "## *Week 1: Importing Datasets*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaaf41b-29b6-4b2e-a798-00ade0ab686f",
   "metadata": {},
   "source": [
    "Why Data Analysis?\n",
    "- data is everywhere.\n",
    "- data analysis & data science help us answer questions from data.\n",
    "- data analysis plays an important role in:\n",
    "    - discovering useful information\n",
    "    - answering questions\n",
    "    - predicting the future or the unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb9af34-c5b3-4810-95cf-d7a944f81896",
   "metadata": {},
   "source": [
    "### Python Packages for Data Science\n",
    "- Scientific Computing Libraries\n",
    "    - Pandas\n",
    "        - Data structures & tools (dataframes)\n",
    "        - Designed to provide easy indexing functionality\n",
    "        - Offers tools and data structure for effective data manipulation and analysis\n",
    "    - Numpy\n",
    "        - Arrays & matrices\n",
    "    - SciPy\n",
    "        - Integrals, solving differential equations, optimization\n",
    "        - Helpful in some data visualization.\n",
    "- Data Visualization Libraries\n",
    "    - Matplotlib\n",
    "        - Plots & graphs, most popular visualization library\n",
    "    - Seaborn\n",
    "        - Plots: heat maps, time series, violin plots\n",
    "        - Based on matplotlib\n",
    "- Algorithmic Libraries\n",
    "    - SciKit-Learn\n",
    "        - Machine learning: statistical modeling, regression, classification, clustering, etc\n",
    "        - Built on Numpy, Scipy, and Matplotlib.\n",
    "    - Statsmodels\n",
    "        - Explore data, estimate statistical models, perform statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f64ed8-e461-4012-a0d0-0ff0b1c9a1a8",
   "metadata": {},
   "source": [
    "### Importing Data in Python\n",
    "\n",
    "Importing:\n",
    "- the process of loading and reading data into Python from various resources\n",
    "- Two important properties include:\n",
    "    - format\n",
    "        - .csv, .json, .xlsx, .hdf, etc\n",
    "    - file path of dataset\n",
    "        - computer: /desktop/mydata.csv\n",
    "        - internet: https//website.com\n",
    "\n",
    "Importing process:\n",
    "1. import pandas library\n",
    "2. store the location of the dataset in a variable\n",
    "3. use the appropriate python pandas.read function on the dataset to create a dataframe\n",
    "4. view the dataframe to double check your work\n",
    "    - df\n",
    "    - df.head()\n",
    "    - df.tail()\n",
    "    \n",
    "Adding headers:\n",
    "- replace default headers with (df.columns = headers), where headers is a variable containing the list of column headers you want.\n",
    "\n",
    "*NOTE: pandas.read_csv assumes there is a header line in the file.  You can set header = None if there isn't one.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa410132-350e-4691-94d7-60fc7784607c",
   "metadata": {},
   "source": [
    "### Exporting Data in Python\n",
    "\n",
    "Exporting:\n",
    "- Preserve progress at any time by saving a modified dataset\n",
    "- Use the appropriate df.to_ method to export to a specific path location.\n",
    "\n",
    "Export process:\n",
    "1. specify the file path, which includes the file name that you want to write to\n",
    "2. use the df.to_csv(path) method\n",
    "\n",
    "| Data Format | Read | Save |\n",
    "|-----------|-------|----------|\n",
    "|csv|pd.read_csv() | df.to_csv()|\n",
    "|json| pd.read_json() | df.to_json()|\n",
    "|Excel| pd.read_excel() | df.to_excel()|\n",
    "|sql | pd.read_sql() | df.to_sql()|\n",
    "|hdf| pd.read_hdf()| df.to_hdf()|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55fe2a6-0d82-4177-902c-f415414c13ab",
   "metadata": {},
   "source": [
    "### Basic insights from the data:\n",
    "- understand your data before you begin any analysis\n",
    "- check for:\n",
    "    - data types\n",
    "        - check for info and potential type mismatches\n",
    "        - check for compatability with python methods\n",
    "        - use df.dtypes to check data types\n",
    "    - data distribution\n",
    "        - return a statistical summary with df.describe() \n",
    "            - skips non-numerical rows automatically, but you can include = 'all' to get summaries for them all\n",
    "        - return a concise summary of the dataframe with df.info()\n",
    "- locate potential issues with the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d629ff7-1121-4eae-82c0-8bc7f12b4045",
   "metadata": {},
   "source": [
    "### Accessing Databases with Python \n",
    "\n",
    "Recall the interaction:\n",
    "- user, jupyter / python programs, API calls, DBMS\n",
    "\n",
    "Concepts of the Python DB API:\n",
    "- Connection Objects\n",
    "    - database connections\n",
    "    - manage transactions\n",
    "- Cursor Objects\n",
    "    - database queries\n",
    "    - Methods\n",
    "        - cursor() \n",
    "            - returns a new cursor object using the connection\n",
    "        - commit()\n",
    "            - commit any pending transactions to the database\n",
    "        - rollback()\n",
    "            - causes the database to rollback to the start of any pending transaction\n",
    "        - close()\n",
    "            - closes the database connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b573d9-2e9e-4d99-9e84-ac37211f0f31",
   "metadata": {},
   "source": [
    "**Example database connection**\n",
    "\n",
    "from dbmodule import connect\n",
    "\n",
    "**create connection object**\n",
    "\n",
    "connection = connect('databasename', 'username', 'password')\n",
    "\n",
    "**create cursor object**\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "**run queries**\n",
    "\n",
    "cursor.execute('select * from mytable')\n",
    "\n",
    "results = cursor.fetchall()\n",
    "\n",
    "**free resources**\n",
    "\n",
    "cursor.close()\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f53b0-1913-44e7-9e68-45a47052c5b2",
   "metadata": {},
   "source": [
    "## *Week 2: Data Wrangling*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87020767-f969-4ddd-a874-10faa956f52c",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "\n",
    "Pre-processing:\n",
    "- The process of converting or mapping data from the initial 'raw' form into another format in order to prepare the data for further analysis.\n",
    "- AKA: data wrangling, data cleaning \n",
    "- This could include:\n",
    "    - handling missing values\n",
    "    - formatting data\n",
    "    - normalizing data\n",
    "    - binning data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00cfe8-1392-41dc-9c7b-1c14d27a6601",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "Missing Values:\n",
    "- occur when no data value is stored for a variable in an observation\n",
    "- could be represented as ?, N/A, 0, or just a blank cell. \n",
    "\n",
    "How to deal with missing data:\n",
    "- check with the data collection source to see if you can find the value\n",
    "- drop the missing values (whichever option has the least amount of impact)\n",
    "    - drop the variable\n",
    "    - drop the data entry\n",
    "    - df.dropna()\n",
    "        - axis = 0 drops an entire row\n",
    "        - axis = 1 drops the entire column\n",
    "        - to modify the dataframe, set inplace = True\n",
    "- replace the missing values\n",
    "    - replace with an average of similar datapoints\n",
    "    - replace it by frequency (mode: the most common value in that column)\n",
    "    - replace it based on other functions\n",
    "    - df.replace(missing_value, new_value)\n",
    "- leave it as missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b437c-99f9-4a24-b06f-0ff1ac442949",
   "metadata": {},
   "source": [
    "### Data Formatting\n",
    "\n",
    "Data Formatting:\n",
    "- Data is usually collected from different places and stored in different formats\n",
    "- Bringing data into a common standard of expression allows users to make meaningful comparisons.\n",
    "- You can update entire columns with calculations.\n",
    "- Update incorrect data types.\n",
    "    - df.dypes() to identify data type\n",
    "    - df.astype() to convert data type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324d443-5316-42a8-836c-2f15ba0d28e3",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "Data Normalization:\n",
    "- Make different variables that have different ranges more uniform and consistent.\n",
    "- Normalization enables a fair comparison between different featuers, making sure that they have the same impact on analysis.\n",
    "\n",
    "Approaches for normalization\n",
    "- Simple Feature Scaling\n",
    "    - ${x}_{new} = {x}_{old}/{x}_{max}$\n",
    "    - This divides each value by the maximum value of that variable, which makes the new values range between 0 and 1.\n",
    "    - df['column'] = df['column']/df[column].max()\n",
    "\n",
    "- Min-Max \n",
    "    - ${x}_{new} = ({x}_{old} - {x}_{min}) /({x}_{max} - {x}_{min})$\n",
    "    - This results in the new values ranging between 0 and 1.\n",
    "    - df['col'] = (df['col'] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "\n",
    "- Z-Score\n",
    "    - ${x}_{new} = ({x}_{old} - \\mu) / \\sigma$ \n",
    "    - The resulting values hover around zero and typically range between [-3, 3].  \n",
    "    - df['col'] = (df['col'] - df['col'].mean())/df['col'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a149fbb-da48-4919-a5b8-0bee0c76f140",
   "metadata": {},
   "source": [
    "### Binning in Python\n",
    "\n",
    "Binning:\n",
    "- the grouping of values into 'bins'\n",
    "- converts numerical into categorical variables\n",
    "- group a set of numerical values into a set of bins\n",
    "     - use bins = np.linspace(min, max, number_of_bins) to return the bins over the specified min, max interval.\n",
    "    - name_variable = [name1, name2, name3...]\n",
    "    - use pd.cut(df[column], bins, labels = name_variable, include lowest=True) to segment and sort the data values into bins.\n",
    "- Once divided into bins, you can create histograms of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a09e9-561d-4618-8846-436e89fc7ff0",
   "metadata": {},
   "source": [
    "### Turning Categorical into Quantitative variables\n",
    "\n",
    "Categorical Variables:\n",
    "- problem: most stats models cannot take in objects or strings as input.\n",
    "- Solution: add dummy variables for each unique category and assign numerical values in each category.\n",
    "    - \"one-hot encoding\"\n",
    "- use pd.get_dummies() method to convert categorical variables to dummy variables (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d767e15-ac44-4935-9bdf-752091d0e4e2",
   "metadata": {},
   "source": [
    "## *Week 3: Exploratory Data Analysis*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836bf2ea-99c1-44de-9788-02cb4aea552a",
   "metadata": {},
   "source": [
    "Exploratory Data analysis (EDA):\n",
    "- Preliminary step in data analysis to:\n",
    "    - summarize main characteristics of the data\n",
    "    - gain better understanding of the data set\n",
    "    - uncover relationships between variables\n",
    "    - extract important variables\n",
    "- What characteristics have the biggest impact on the question we want to solve / data we want to understand?\n",
    "\n",
    "Useful Tools:\n",
    "- Descriptive Statistics: describe basic features of the data set and obtain a short summary about the sample and measures of the data. \n",
    "- GroupBy: grouping of data and how it can help transform the data\n",
    "- Correlation between variables\n",
    "- Statistical correlation methods\n",
    "    - pearson correlation\n",
    "    - correlation heat maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbec7b9-a311-467b-ad78-c4dd6c465e23",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "\n",
    "Descriptive Statistics:\n",
    "- Describe basic features of data\n",
    "- Give short summaries about the sample and measure of the data\n",
    "\n",
    "Useful methods for descriptive statistics:\n",
    "- df.describe()\n",
    "    - computes basic statistics for all numeric variables in the dataframe\n",
    "        - mean, total number of data points, standard deviation, quartiles, and extreme values. \n",
    "        - NaN values are skipped\n",
    "\n",
    "\n",
    "- df.value_counts()\n",
    "    - can be used to summarize categorical data by dividing the data into discrete groups to be counted\n",
    "\n",
    "\n",
    "    \n",
    "- box plots: sns.boxplot(x = 'xrange', y = 'yrange', data = df)\n",
    "    - visualize the median, quartiles, extremes, and outliers\n",
    "\n",
    "\n",
    "- scatter plots: plt.scatter(x = df['col1'], y = df['col2']) \n",
    "    - each observation is represented as a point\n",
    "    - show the relationship between two variables\n",
    "        - predictor/independent variable (x-axis) is used to predict an outcome\n",
    "        - target/dependent variable (y-axis) is the variable you are trying to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f00ce-c131-4ad1-885d-279810630f3f",
   "metadata": {},
   "source": [
    "### GroupBy in Python\n",
    "\n",
    "Grouping data:\n",
    "- use Pandas df.Groupby() method:\n",
    "    - can be applied to categorical variables\n",
    "    - group data into categories\n",
    "    - can be used on single or multiple variables\n",
    "    \n",
    "Pivot tables:\n",
    "- Use Pandas method df.Pivot() method:\n",
    "    - one variable is displayed along the columns and the other variable is displayed along the rows\n",
    "    - Helps make grouped data easier to understand and visualize\n",
    "    \n",
    "Heatmaps:\n",
    "- Use plt.pcolor(df_pivot, cmap = 'RdBu') method:\n",
    "    - Plot target variables over multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83df364-6b39-4a47-89e7-599eda8edfda",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "Correlation:\n",
    "   - a statistical metric for measure to what extent different variables are interdependent.\n",
    "  \n",
    "  - Correlation does not imply causation.\n",
    "    \n",
    "   - Correlation can be positive or negative.\n",
    "   \n",
    "  - Correlation can be weak (almost flat, graphically) or strong (steep slope, graphically)\n",
    "   \n",
    "   - You can visualize the correlation between two variables using a scatter plot with a linear regression line, which indicates the relationship between the two. \n",
    "    \n",
    "    - sns.regplot(x = , y = , data = df)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f62b33-45ae-43ec-876d-02ba329ec786",
   "metadata": {},
   "source": [
    "### Pearson Correlation\n",
    "\n",
    "Correlation Statistical Methods for numeric variables:\n",
    "- Pearson Correlation\n",
    "    - use SciPy's stats package to calculate\n",
    "        - pearson_coef, p_value = stats.pearsonr(df['col'], df['col2'])\n",
    "    - one  way to measure the strength of the correlation between continuous numerical variables.\n",
    "    - gives two values: \n",
    "        - correlation coefficient\n",
    "            - a value close to 1 implies strong positive correlation\n",
    "            - a value close to -1 implies strong negative correlation\n",
    "        - P-value\n",
    "            - tells us how certain we are about the correlation that we calculated.\n",
    "            - a value less than .001 gives strong certainty about the correlation\n",
    "            - a value between .001 and .05 gives moderate certainty\n",
    "            - a value between .05 and .1 gives weak certainty\n",
    "            - a value larger than .1 gives no certainty at all\n",
    "    - you can say that there is a strong correlation when the correlation coefficient is close to +/-1 and the p-value is less than .001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783639d-355f-4a23-8e05-6ee01683fda0",
   "metadata": {},
   "source": [
    "### Chi-Square Test\n",
    "Assocation between two categorical variables:\n",
    "- Chi-square test for Association \n",
    "    - intended to test how likely it is that an observed distribution is due to chance.\n",
    "    - it measures how well the observed distribution of data fits with the distribution that is expected if the variables are independent\n",
    "    - it tests a null hypothesis that the variables are independent.\n",
    "    - the test then compares the observed data to the values that the model expects if the data was distributed in different categories by chance. \n",
    "    - Any time the observed data doesn't fit within the model of the expected values, the probability that the variables are dependent becomes stronger, thus proving the null hypothesis incorrect. \n",
    "    - This test does not tell you the type of relationship that exists, only that it exists.\n",
    "    - you can first find the observed counts of the variable(s) using a crosstab from the pandas library (aka, a contingency table if the relationship is between two categorical variables)\n",
    "        - sum((observed value - expected value)^2 /expected value \n",
    "        - expected value: (row total * column total) / grand total\n",
    "        - degree of freedom = (row-1)*(column-1)\n",
    "        - if the resulting value in the chi-square table is <.05, you can reject the null hypothesis that the variables are independent and conclude there is a correlation.\n",
    "- In python, you can use the chi square contingency function in the scipy.stats package.\n",
    "    - scipy.stats.chi2_contingency(cont_table, correction = True)\n",
    "        - results in [chi-square test value, p-value, degree of freedom] as well as an array with the values of each expected value in the contingency table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683e5949-f3f4-49e1-bc50-ccf7f49d9d67",
   "metadata": {},
   "source": [
    "## *Week 4: Model Development*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d389b913-a6ac-4bed-95c5-4e63dcda792a",
   "metadata": {},
   "source": [
    "### Model Development\n",
    "\n",
    "Model:\n",
    "- A model can be thought of as a mathematical equation used to predict a value given one or more other values.\n",
    "- It relates one or more independent variables to dependent variables.\n",
    "- The more relevant data you have, the more accurate your model is usually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f348f3-3323-463e-994a-faa032dd0850",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Simple Linear Regression (SLR):\n",
    "- Linear regression refers to one independent variable to make a prediction.\n",
    "- A method to help understand the relationship between two variables, the predictor/independent variable x and target/dependent variable y.\n",
    "- We want to create a linear relationship between the below variables.\n",
    "    - ${y} = {b}_{0} + {b}_{1}{x}$\n",
    "        - ${b}_{0}$ : the intercept\n",
    "        - ${b}_{1}$ : the slope\n",
    "- Fit: we take several training points of the relationship to train the model. The results of the training points are the parameters, usually stored into a dataframe or numpy array. \n",
    "- Noise: In most cases, many factors influence these variables. This uncertainty is taken into account by assuming a small random value is added to the point on the line. This is called noise.\n",
    "- Once you have the model set up and running $\\hat{y}$ is used to indicate that this is an estimate/prediction of the value of y. We can then use this model to predict values we haven't yet seen.\n",
    "\n",
    "Linear Regression Model in Python:\n",
    "1. import linear_model from scikit-learn\n",
    "    - from sklearn.linear_model import LinearRegression\n",
    "2. create linear regression object using the constructor:\n",
    "    - lm = LinearRegression()\n",
    "3. define the predictor and target variables\n",
    "    - x = df[['col']]\n",
    "    - y = df[col]\n",
    "4. use lm.fit to fit the model, i.e. find the parameters.\n",
    "    - lm.fit(x,y)\n",
    "5. obtain a prediction in an array\n",
    "    - yhat = lm.predict(x)\n",
    "6. view the intercept:\n",
    "    - lm.intercept_\n",
    "7. view the slope:\n",
    "    - lm.coef_\n",
    "8. The relationship between the variables is given by:\n",
    "    - ${y} = {b}_{0} + {b}_{1}{x}$\n",
    "    \n",
    "    \n",
    "Define the response variable (y) as the focus of the experiment and the explanatory variable (x) as a variable used to explain the change of the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e5044b-667f-4e53-ae63-61a604ab6d64",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression:\n",
    "- Multiple linear regression refers to multiple independent variables to make a prediction\n",
    "- used to explain the relationship between one continuous target (y) variable and two+ predictor (x) variables\n",
    "    - for example, the linear relationship of four predictor variables would look like:\n",
    "        - ${\\hat{y}} = {b}_{0} + {b}_{1}{x}_{1}+ {b}_{2}{x}_{2}+ {b}_{3}{x}_{3} +  {b}_{4}{x}_{4}$\n",
    "        \n",
    "Multiple Linear Regression in Python:\n",
    "1. import linear_model from scikit-learn\n",
    "    - from sklearn.linear_model import LinearRegression\n",
    "2. create linear regression object using the constructor:\n",
    "    - lm = LinearRegression()\n",
    "3. Extract the predictor variables and store them into a variable\n",
    "    - z = df[['col1', 'col2', 'col3', 'col4']]\n",
    "4. train the model \n",
    "    - lm.fit(z, df['ycolumn'])\n",
    "5. obtain a prediction\n",
    "    - yhat = lm.predict(x)\n",
    "6. find intercept\n",
    "    - lm.intercept_\n",
    "7. find coefficients\n",
    "    - lm.coef_\n",
    "8. Find the relationship\n",
    "    \n",
    "*NOTE: it is helpful to replace the dependent variable names with actual names to keep their values readable.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c23bc-c84b-4018-aa1f-c8f442ae4444",
   "metadata": {},
   "source": [
    "### Model Evaluation using Visualization\n",
    "\n",
    "Regression Plot:\n",
    "- it gives a good estimate of:\n",
    "    - the relationship between two variables\n",
    "    - the strength of the correlation\n",
    "    - the direction of the relationship (positive, negative)\n",
    "- The regression plot shows a combination of:\n",
    "    - the scatterplot, where each point represents a different y\n",
    "    - the fitted linear regression line ($\\hat{y}$)\n",
    "- How to create:\n",
    "    - import seaborn as sns\n",
    "    - sns.regplot(x = independentvalue , y = dependentvalue, data = )\n",
    "    - plt.ylim(0,)\n",
    "    \n",
    "Residual Plot:\n",
    "- represents the error between the actual value and the predicted value.\n",
    "- This difference value is calculated and then plotted on the vertical axis with the independent variable as the horizontal.\n",
    "- We expect to see that the results have zero mean; i.e., they are distributed evenly around the x axis with similar variance. This indicates that a linear plot is appropriate.\n",
    "- If the residual plot points aren't evenly spread, it indicates a linear plot is not appropriate and a nonlinear model may fit better.\n",
    "- How to create:\n",
    "    - import seaborn as sns\n",
    "    - sns.residualplot(df[dependentvariable], df[independentvariable])\n",
    "    \n",
    "Distribution Plot:\n",
    "- Counts the predicted value versus the actual value. \n",
    "- Very useful for visualizing models with more than one independent variable.\n",
    "- Count the plot the number of predicted points that are approximately equal to each bin of values.\n",
    "- Then repeat the process for the number of actual points that are approximately equal to each bin of values.\n",
    "- Pandas will convert this histogram into a distribution (as histograms are only for discrete values).\n",
    "- This graphically shows how accurate/inaccurate your  model is within certain ranges.\n",
    "- How to create:\n",
    "    - import seaborn as sns\n",
    "    - ax1 = sns.distplot(df['actual values'], hist = False, color = 'r', label = \"actual value')\n",
    "    - sns.distplot(yhat, hist = False, color = 'b', label = \"fitted values\", ax = ax1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631a0993-682f-48e6-b6d1-1faa295bb6c1",
   "metadata": {},
   "source": [
    "### Polynomial Regression and Pipelines\n",
    "\n",
    "Polynomial Regression:\n",
    "- a special case of the general linear regression model\n",
    "- useful for describing curvilinear relationships\n",
    "    - Curvilinear relationships occur by squaring or setting higher-order terms of the predictor variables\n",
    "    - Example: Quadratic/2nd order\n",
    "        - ${\\hat{y}} = {b}_{0} + {b}_{1}({x}_{1})^1+ {b}_{2}({x}_{1})^2$\n",
    "    - Example: cubic / 3rd order\n",
    "        - ${\\hat{y}} = {b}_{0} + {b}_{1}({x}_{1})^1+ {b}_{2}({x}_{1})^2+ {b}_{3}({x}_{1})^3$\n",
    "- The graphs change so much as you go higher and higher in degree. It can result in a better fit if you chose the correct value.\n",
    "- In all cases, the relationship between the variable and parameter is always linear.\n",
    "- You can also have multi dimensional polynomial linear regression:\n",
    "    - Numpy's polyfit function cannot perform this, so instead use the preprocessing library in scikit learn.\n",
    "    - ${\\hat{y}} = {b}_{0} + {b}_{1}{x}_{1}+ {b}_{2}{x}_{2}+ {b}_{3}{x}_{1}{x}_{2} + {b}_{4}({x}_{1})^2 + {b}_{5}({x}_{2})^2 +...$\n",
    "\n",
    "How to create polynomial regression:\n",
    "1. calculate polynomial of zth order:\n",
    "    - f.np.polyfit(x, y, z)\n",
    "    - p = np.poly1d(f)\n",
    "2. print out the model:\n",
    "    - print(p)\n",
    "    \n",
    "How to create multidimensional polynomial linear regression:\n",
    "1. import preprocessing library\n",
    "    - from sklearn.preprocessing import PolynomialFeatures\n",
    "2. Create a polynomialfeature object\n",
    "    - pr = polynomialfeatures(degree = 2, include_bias = False)\n",
    "3. Transform the features into a polynomialfeature with the fit_transform method\n",
    "    - x_polly = pr.fit_transform(x[['col1', 'col2']])\n",
    "4. as the dimensions get larger, we may want to normalize multiple features in scikit-learn:\n",
    "    - import library\n",
    "        - from sklearn.preprocessing import StandardScaler\n",
    "    - train the object\n",
    "        - SCALE = StandardScaler()\n",
    "    - fit the scale object\n",
    "        - SCALE.fit(x_data[['col1', 'col2']])\n",
    "    - transform the data into a new dataframe\n",
    "        - x_scale = SCALE.transform(x_data[['col1', 'col2']])\n",
    "\n",
    "Pipelines:\n",
    "- There are many steps to getting a prediction\n",
    "- Pipelines can help simplify the code by performing a series of transformations before the last step, which carries out the prediction.\n",
    "- This method:\n",
    "    - normalizes the data, performs a polynomial transformation, and outputs a prediction.\n",
    "\n",
    "How to set up pipelines:\n",
    "1. Import all needed modules\n",
    "    - from sklearn.preprocessing import PolynomialFeatures\n",
    "    - from sklearn.linear_model import LinearRegression\n",
    "    - from sklearn.preprocessing import StandardScaler\n",
    "    - from sklearn.pipeline import Pipeline\n",
    "2. Create a list of tuples that contain the (estimatorModelName, ModelConstructor)\n",
    "    - input = [('scale', StandardScaler()), ('polynomial', PolynomialFeatures(degree = 2), ...., ('mode', LinearRegression())]\n",
    "3. Input the list into the pipeline constructor to get a pipeline object\n",
    "    - pipe = Pipeline(input)\n",
    "4. train the pipeline object\n",
    "    - pipe.fit(df[['col1', ... , 'coln']], y)\n",
    "5. Produce a prediction\n",
    "    - yhat = pipe.prediction(x[['col1', ...., 'col4']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde5ed5-cf76-46ac-b503-dad2e0f8af5b",
   "metadata": {},
   "source": [
    "### Measures for In-Sample Evaluation\n",
    "\n",
    "Now that we've seen how we can evaluate a model via visualization, we want to numerically evaluate models.\n",
    "\n",
    "Measures used for in-sample evaluation:\n",
    "- used to numerically determined how good the model fits on the dataset\n",
    "- Two important measures to determine the fit of a model:\n",
    "    - mean squared error (MSE)\n",
    "    - R-Squared ($R^2$)\n",
    "    \n",
    "MSE:\n",
    "- find the difference between actual value y and predicted value y, then square it. \n",
    "- Then take the mean or average of all the errors by adding them all together and dividing by the number of samples. \n",
    "- MSE tells you how close a regression line is to a set of points.\n",
    "\n",
    "Find MSE in Python:\n",
    "1. import library\n",
    "    - from sklearn.metrics import mean_squared_error\n",
    "2. Use the mean squared function\n",
    "    - mean_squared_error(df[yvariable], y_predict_simple_fit)\n",
    "    \n",
    "R-Squared:\n",
    "- AKA the coefficient of determination \n",
    "- is a measure to determine how close the data is to the fitted regression line\n",
    "- Interpret R-Square (x100) as the percentage of the variation in response variable y that is explained by the variation in explanatory variables x.\n",
    "- $R^2$: the percentage of variation of the target variable (Y) that is explained by the linear model.\n",
    "- $R^2$ = 1- ((MSE of regression line)/(MSE of average of data))\n",
    "- Usually takes on values between (0, 1)\n",
    "- If the line is a good fit, the MSE of the regression line will be small and the MSE of the average of the data will be somwhat larger\n",
    "    - meaning that $R^2$ = 1 - (a number very close to zero)\n",
    "- If $R^2$ is close to zero, the line did not perform well. \n",
    "- If $R^2$ is negative, it can be due to overfitting.\n",
    "\n",
    "Find R-Squared in Python:\n",
    "1. plug in x and y\n",
    "    - x = df[[col]]\n",
    "    - y = df[col2]\n",
    "2. use lm.fit \n",
    "    - lm.fit(x,y)\n",
    "3. use lm.score to get the value of r-squared\n",
    "    - lm.score(x,y)\n",
    "    - say this number comes out to be .46591188.  Then you can say that approximately 46% of the variation of price is explained by the simple linear model.  The higher the score, the better the model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f1d65-6030-4bcb-8239-579d176e270d",
   "metadata": {},
   "source": [
    "### Prediction and Decision Making\n",
    "\n",
    "Determining a good model fit:\n",
    "- look at a combination of:\n",
    "    - do the predicted values make sense?\n",
    "    - visualization\n",
    "    - numerical measures for evaluation\n",
    "    - comparing models\n",
    "    \n",
    "Generate a sequence of values in a specified range to get some predicted values\n",
    "- import numpy as np\n",
    "- new_input = np.arange(1, 101, 1).reshape(-1, 1)\n",
    "    - starts at 1 and increments by 1 until hitting 100\n",
    "- yhat = lm.predict(new_input)\n",
    "- Does the output make sense?\n",
    "- Visualize the data to understand it better.\n",
    "- test the numerical measures.\n",
    "- \n",
    "\n",
    "Comparing Multiple Linear Regression and Simple Linear Regression:\n",
    "- does a lower Mean Square Error imply better fit?\n",
    "    - not necessarily\n",
    "    - MSE for MLR will be smaller than MSE for SLR since the errors of the data will decrease when more variables are included in the model.\n",
    "    - polynomial regression will also have a smaller MSE than regular linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c7cf6-555b-4245-ae38-6b96567643bd",
   "metadata": {},
   "source": [
    "## *Week 5: Model Evaluation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b32bb3-c094-45fd-bd3e-e7282e98bb27",
   "metadata": {},
   "source": [
    "### Model Evaluation and Refinement\n",
    "\n",
    "Model Evaluation\n",
    "- In-sample evaluation tells us how well the model will fit the data used to train it.\n",
    "- Problem: in-sample evaluation does not tell us how well the trained model can be used to predict new data.\n",
    "- Solution: \n",
    "    - in-sample data or training data\n",
    "    - out of sample evaluation or test set\n",
    "- Separating data into training and testing sets is an important part of model evaluation. \n",
    "- Split data into:\n",
    "    - Training set (estimate 70%)\n",
    "    - Testing set (estimate 30%)\n",
    "- Build and train the model with the training set\n",
    "- Use testing set to asses the performance of a predictive model\n",
    "- When we have completed testing the model, we should use all the data to train the model to get the best performance.\n",
    "\n",
    "Splitting Data Sets in Python:\n",
    "- SciKit-learn function train_test_split()\n",
    "    - splits data into random train and test subsets\n",
    "- Syntax: \n",
    "    - from sklearn.model_selection import train_test_split\n",
    "    - x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = .3, random_state = 0)\n",
    "        - x_data: features or independent variables\n",
    "        - y_data: dataset target (such as price)\n",
    "        - x_test, y_test: parts of available data as testing set\n",
    "        - test_size: percentage of data used for testing\n",
    "        - random_state: number generator used for random sampling\n",
    "        \n",
    "Generalization performance:\n",
    "- Generalization error is a measure of how well the data does at predicting previously unseen data\n",
    "- The error we obtain using the testing data is an approximation of this error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c1b64-6517-49f1-969d-9bd64ff6de76",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "Cross Validation:\n",
    "- Most common out of sample evaluation metrics\n",
    "- More effective use of data (each observation is used for both training and testing)\n",
    "- The data is divided into 'folds', with a portion of the folds used for training and the others used for testing.  \n",
    "- The process is repeated until each of the folds has had the opportunity to be both a test and train fold. \n",
    "- Then the average results are used as an estimate of the out-of-sample error.\n",
    "- In Python use cross_val_score():\n",
    "    - from sklearn.model_selection import cross_val_score\n",
    "    - scores = cross_val_score(lr, x_data, y_data, cv=3)\n",
    "        - lr: linear regression model chosen here, but other model types can be used\n",
    "        - x_data: the predictive variable data\n",
    "        - y_data: the target variable data\n",
    "        - cv: how many folds / splits we have in the data.\n",
    "    - This function returns an array of scores, one for each partition that was chosen as the testing set.  \n",
    "    - We can average the result together to estimate out of sample r-squared using the mean function of NumPy\n",
    "        - np.mean(scores)\n",
    "- If we want to know the actual predicted values supplied by the model, use cross_val_predict()\n",
    "    - it returns the prediction that was obtained for each element when it was in the test set.\n",
    "    - Has a similar interface to cross_val_score()\n",
    "    - from sklearn.model_selection import cross_val_predict\n",
    "    - yhat = cross_val_predict(lr2e, x_data, y_data, cv = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ffd827-40a5-4df9-ba21-9a60d0420ab7",
   "metadata": {},
   "source": [
    "### Overfitting and Underfitting\n",
    "\n",
    "This section is specifically in regards to polynomial regression and how to pick the best polynomial order for your model.\n",
    "\n",
    "Underfitting:\n",
    "- The model is too simplistic to fit the data\n",
    "\n",
    "Overfitting: \n",
    "- The model does well at tracking training points but performs poorly at estimating the function. This is especially apparent where there is little training data, where the estimated function will oscillate and not track the function probperly.\n",
    "- The model is too flexible and fits the noise rather than the function.\n",
    "\n",
    "Model Selection:\n",
    "- On a graph comparing the Error MSE vs the order of the polynomial, we note:\n",
    "- Training error decreases as the order of the polynomial goes up.\n",
    "- Test error is a better means fo estimating the error of a polynomial.  This error decreases until the best order of the polynomial is determined, then increases.\n",
    "- Select the error that best minimizes the test error.  \n",
    "- Anything on the \"left\" of this graph is underfitting, while everything on the \"right\" is overfitting.\n",
    "- Even the best choice for polynomial order will still have some error due to noise and other issues that is unavoidable.\n",
    "\n",
    "![graph in question](https://vitalflux.com/wp-content/uploads/2020/12/overfitting-and-underfitting-wrt-model-error-vs-complexity.png)\n",
    "\n",
    "Calculate different R-squared values in Python:\n",
    "- Rsqu_test[]\n",
    "- order = [1, 2, 3, 4]\n",
    "- for n in order:\n",
    "    - pr = POlynomialFeatures(degree = n)\n",
    "    - x_train_pr=pr.fit_transform(x_train[['col']])\n",
    "    - x_test_pr = pr.fit_transform(x_test[['col']])\n",
    "    - lr.fit(x_train_pr, y_train\n",
    "    - rsqu_test.append(lr.score(x_test_pr, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0027571-ff3c-4d5c-aaf0-548aab35a39e",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Ridge Regression:\n",
    "- a regression that is employed in a multiple regression model when Multicollinearity occurs. \n",
    "- Multicollinearity is when there is a strong relationship among the independent variables. \n",
    "- Ridge regression is very common with polynomial regression.\n",
    "- Ridge regression helps prevent overfitting, which is a big problem when you have multiple independent variables.\n",
    "- Ridge regression helps to control the magnitude of the polynomial coefficients by introducing the parameter alpha, which is a parameter we select prior to fitting or training the model. \n",
    "- if alpha is too large, the coefficients will approach zero and underfit the data.\n",
    "- if alpha is zero, overfitting becomes a problem.\n",
    "- Select the correct alpha using cross validation:\n",
    "    - from sklearn.linear_model import Ridge\n",
    "    - RidgeModel = Ridge(alpha = .1)\n",
    "    - RidgeModel.fit(X,y)\n",
    "    - yhat = RidgeModel.predict(X)\n",
    "- To make a prediction, use the predict method. In order to determine alpha, use some data for training and a second set called validation data (which is similar to test data, but used to select parameters).\n",
    "- Start with a small value of alpha, train the model, make a prediction using the validation data, and calculate the R-squared and store the values.\n",
    "- Repeat this process for larger values of Alpha.\n",
    "- Select the value of Alpha that maximizes R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878044b-0523-481d-8e97-5f8a9077022c",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "Grid Search:\n",
    "- Allows us to scan through multiple free parameters with few lines of code. \n",
    "- Hyperparameters:\n",
    "    - alpha is a hyperparameter\n",
    "    - Scikit-learn has a means of automatically iterating over these hyperparameters using cross-validation called Grid Search.\n",
    "- Grid search takes the model or objects you want to train and different values of hyperparamters. It then calculates the MSE or R-squared for various hyperparameter values, allowing you to choose the best values.\n",
    "- Select the hyperparameter that results in the smallest error out of  all the grid search validation result errors.\n",
    "- syntax:\n",
    "    - from sklearn.linear_model import Ridge\n",
    "    - from sklearn.model_selection import GridSearchCV\n",
    "    - parameters = [{'alpha': [.001, .1, 1, 10, 100, 1000, 10000, 100000, 1000000], 'normalize':[True, False]}]\n",
    "    - RR = Ridge()\n",
    "    - Grid1 = GridSearchCV(RR, parameters, cv = 4)\n",
    "    - Grid1.fit(x_data[['col1', 'col2', 'col3'...]], y_data)\n",
    "    - Grid1.best_estimator_\n",
    "    - scores = Grid1.cv_results_\n",
    "    - scores['mean_test_score']\n",
    "- Note: you can use the normalize parameter to help you decide if normalization is a good choice for your model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
